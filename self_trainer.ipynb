{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "a4ca0a1be38340d2e1b70bc6cc73081324f38c92d6b10994d2eeb96728463324"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense,Flatten,Activation,Dropout,Maximum,ZeroPadding2D,Conv2D,MaxPooling2D,concatenate,Add,BatchNormalization,LeakyReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize as imresize\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layers set\n",
    "def dense_set(inp_layer, n, activation, drop_rate=0.):\n",
    "    dp = Dropout(drop_rate)(inp_layer)\n",
    "    dns = Dense(n)(dp)\n",
    "    bn = BatchNormalization(axis=-1)(dns)\n",
    "    act = Activation(activation=activation)(bn)\n",
    "    return act\n",
    "\n",
    "# Conv. layers set\n",
    "def conv_layer(feature_batch, feature_map, kernel_size=(3, 3),strides=(1,1), zp_flag=False):\n",
    "    if zp_flag:\n",
    "        zp = ZeroPadding2D((1,1))(feature_batch)\n",
    "    else:\n",
    "        zp = feature_batch\n",
    "    conv = Conv2D(filters=feature_map, kernel_size=kernel_size, strides=strides,padding='same')(zp)\n",
    "    bn = BatchNormalization(axis=3)(conv)\n",
    "    act = LeakyReLU(1/10)(bn)\n",
    "    return act\n",
    "def residual_block(feature_batch, feature_map,stride=(1,1)):\n",
    "    res=conv_layer(feature_batch, feature_map, kernel_size=(3, 3),strides=stride, zp_flag=False)\n",
    "    res=conv_layer(res, feature_map, kernel_size=(3, 3),strides=(1,1), zp_flag=False)\n",
    "    shortcut=Conv2D(filters=feature_map, kernel_size=(3,3), strides=stride,padding='same')(feature_batch)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    output=Add()([shortcut,res])\n",
    "    output=LeakyReLU(1/10)(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model \n",
    "def get_model():\n",
    "    inp_img = Input(shape=(128, 64, 3))\n",
    "\n",
    "    # 51\n",
    "    conv1 = residual_block(inp_img, 32)\n",
    "    conv2 = residual_block(conv1, 32)\n",
    "    #mp1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2),padding='same')(conv2)      #64*32\n",
    "    # 23\n",
    "    conv3 = residual_block(conv2, 64,stride=(2,2))\n",
    "    conv4 = residual_block(conv3, 64)\n",
    "    #mp2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2),padding='same')(conv4)      #32*16\n",
    "    # 9\n",
    "    conv7 = residual_block(conv4, 128,stride=(2,2))\n",
    "    conv8 = residual_block(conv7, 128)\n",
    "    conv9 = residual_block(conv8, 128)\n",
    "    mp3 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2),padding='same')(conv9)      #16*8\n",
    "    # 1\n",
    "    # dense layers\n",
    "    flt=tf.keras.layers.GlobalAveragePooling2D()(mp3)\n",
    "    #flt = Flatten()(mp3)\n",
    "    ds1 = dense_set(flt, 128, activation='tanh')\n",
    "    feature=tf.math.l2_normalize(ds1, axis=1)\n",
    "    out = dense_set(ds1, 1500, activation='softmax')\n",
    "\n",
    "    model = Model(inputs=inp_img, outputs=out)\n",
    "    \n",
    "    # The first 50 epochs are used by Adam opt.\n",
    "    # Then 30 epochs are used by SGD opt.\n",
    "    \n",
    "    #mypotim = Adam(lr=2 * 1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    mypotim = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=mypotim,\n",
    "                   metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I trained model about 12h on GTX 950.\n",
    "def train_model(img, target):\n",
    "    my_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model/model.{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    gmodel = get_model()\n",
    "    gmodel.load_weights(filepath='model/model.47-1.40.hdf5')    #載入預先訓練的權重\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "                                                        img,\n",
    "                                                        target,\n",
    "                                                        shuffle=True,\n",
    "                                                        train_size=0.8,\n",
    "                                                        random_state=1987\n",
    "                                                        )\n",
    "    gen = ImageDataGenerator(\n",
    "            #rotation_range=360.,\n",
    "            #width_shift_range=0.3,\n",
    "            #height_shift_range=0.3,\n",
    "            #zoom_range=0.3,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True\n",
    "    )\n",
    "    #show augumented images\n",
    "    '''\n",
    "    show_img=image.img_to_array(x_train[0])\n",
    "    show_img=show_img.reshape((1,)+show_img.shape)\n",
    "    i=0\n",
    "    for batch in gen.flow(show_img,save_prefix='test',save_format='jpeg'):\n",
    "            plt.figure(i)\n",
    "            plot=plt.imshow(image.img_to_array(batch[0]))\n",
    "            i+=1\n",
    "            if i>4:\n",
    "                break\n",
    "    plt.show()\n",
    "    '''\n",
    "    gmodel.fit(gen.flow(x_train, y_train,batch_size=32),\n",
    "               steps_per_epoch=len(x_train)/32,\n",
    "               epochs=47,\n",
    "               verbose=1,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_valid, y_valid),\n",
    "               callbacks=my_callbacks\n",
    "               )\n",
    "    #gmodel.save(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Resize all image to 128x64 \n",
    "def img_reshape(img):\n",
    "    img = imresize(img, (128, 64, 3))\n",
    "    return img\n",
    "\n",
    "# get image tag\n",
    "def img_label(path):\n",
    "    path=path.split('/')[-1]\n",
    "    return int(re.split(r'\\\\',path)[-2])\n",
    "\n",
    "# fill train and test dict\n",
    "def fill_dict(paths, some_dict):\n",
    "    text = ''\n",
    "    text = 'Start fill train_dict'\n",
    "\n",
    "    for p in tqdm(paths, ascii=True, ncols=85, desc=text):\n",
    "        img = imageio.imread(p)\n",
    "        img = img_reshape(img)\n",
    "        some_dict['image'].append(img)\n",
    "        some_dict['label'].append(img_label(p))\n",
    "\n",
    "    return some_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image from dir and fill train and test dict\n",
    "def reader(dir='dataset'):\n",
    "    file_ext = []\n",
    "    train_path = []\n",
    "    test_path = []\n",
    "\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        if dirs != []:\n",
    "            print('Root:\\n'+str(root))\n",
    "            print('Dirs:\\n'+str(dirs))\n",
    "        else:\n",
    "            for f in files:\n",
    "                ext = os.path.splitext(str(f))[1][1:]\n",
    "\n",
    "                if ext not in file_ext:\n",
    "                    file_ext.append(ext)\n",
    "                path = os.path.join(root, f)\n",
    "                train_path.append(path)\n",
    "    train_dict = {\n",
    "        'image': [],\n",
    "        'label': []\n",
    "    }\n",
    "    train_dict = fill_dict(train_path, train_dict)\n",
    "    return train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "                                                      leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)       (None, 128, 64, 32)  0           add_1[0][0]                      \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 64, 32, 64)   18496       leaky_re_lu_5[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 64, 32, 64)   256         conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)       (None, 64, 32, 64)   0           batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 64, 32, 64)   36928       leaky_re_lu_6[0][0]              \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 64, 32, 64)   18496       leaky_re_lu_5[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 64, 32, 64)   256         conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 64, 32, 64)   256         conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)       (None, 64, 32, 64)   0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 64, 32, 64)   0           batch_normalization_8[0][0]      \n                                                                 leaky_re_lu_7[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_8 (LeakyReLU)       (None, 64, 32, 64)   0           add_2[0][0]                      \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 64, 32, 64)   36928       leaky_re_lu_8[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 64, 32, 64)   256         conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_9 (LeakyReLU)       (None, 64, 32, 64)   0           batch_normalization_9[0][0]      \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 64, 32, 64)   36928       leaky_re_lu_9[0][0]              \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 64, 32, 64)   36928       leaky_re_lu_8[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 64, 32, 64)   256         conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 64, 32, 64)   256         conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_10 (LeakyReLU)      (None, 64, 32, 64)   0           batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 64, 32, 64)   0           batch_normalization_11[0][0]     \n                                                                 leaky_re_lu_10[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_11 (LeakyReLU)      (None, 64, 32, 64)   0           add_3[0][0]                      \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 32, 16, 128)  73856       leaky_re_lu_11[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 32, 16, 128)  512         conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_12 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 32, 16, 128)  73856       leaky_re_lu_11[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 32, 16, 128)  512         conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 32, 16, 128)  512         conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_13 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\nadd_4 (Add)                     (None, 32, 16, 128)  0           batch_normalization_14[0][0]     \n                                                                 leaky_re_lu_13[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_14 (LeakyReLU)      (None, 32, 16, 128)  0           add_4[0][0]                      \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_14[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 32, 16, 128)  512         conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_15 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_15[0][0]             \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_14[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 32, 16, 128)  512         conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 32, 16, 128)  512         conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_16 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\nadd_5 (Add)                     (None, 32, 16, 128)  0           batch_normalization_17[0][0]     \n                                                                 leaky_re_lu_16[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_17 (LeakyReLU)      (None, 32, 16, 128)  0           add_5[0][0]                      \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_17[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 32, 16, 128)  512         conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_18 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_18[0][0]     \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_18[0][0]             \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_17[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 32, 16, 128)  512         conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 32, 16, 128)  512         conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_19 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_19[0][0]     \n__________________________________________________________________________________________________\nadd_6 (Add)                     (None, 32, 16, 128)  0           batch_normalization_20[0][0]     \n                                                                 leaky_re_lu_19[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_20 (LeakyReLU)      (None, 32, 16, 128)  0           add_6[0][0]                      \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 16, 8, 128)   0           leaky_re_lu_20[0][0]             \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 128)          0           max_pooling2d[0][0]              \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 128)          0           global_average_pooling2d[0][0]   \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 128)          16512       dropout[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_21 (BatchNo (None, 128)          512         dense[0][0]                      \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 128)          0           batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 128)          0           activation[0][0]                 \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1500)         193500      dropout_1[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_22 (BatchNo (None, 1500)         6000        dense_1[0][0]                    \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 1500)         0           batch_normalization_22[0][0]     \n==================================================================================================\nTotal params: 1,627,724\nTrainable params: 1,621,012\nNon-trainable params: 6,712\n__________________________________________________________________________________________________\nEpoch 1/47\n  2/456 [..............................] - ETA: 1:56 - loss: 7.8517 - accuracy: 0.0000e+00WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.184543). Check your callbacks.\n456/456 [==============================] - 84s 185ms/step - loss: 6.6372 - accuracy: 0.0157 - val_loss: 6.2802 - val_accuracy: 0.0195\nEpoch 2/47\n456/456 [==============================] - 78s 170ms/step - loss: 5.8007 - accuracy: 0.0450 - val_loss: 6.1535 - val_accuracy: 0.0233\nEpoch 3/47\n456/456 [==============================] - 78s 171ms/step - loss: 5.3242 - accuracy: 0.0742 - val_loss: 5.6275 - val_accuracy: 0.0455\nEpoch 4/47\n456/456 [==============================] - 77s 170ms/step - loss: 4.9066 - accuracy: 0.1124 - val_loss: 5.8160 - val_accuracy: 0.0414\nEpoch 5/47\n456/456 [==============================] - 75s 165ms/step - loss: 4.5322 - accuracy: 0.1502 - val_loss: 4.8199 - val_accuracy: 0.1187\nEpoch 6/47\n456/456 [==============================] - 78s 171ms/step - loss: 4.1929 - accuracy: 0.1876 - val_loss: 4.3105 - val_accuracy: 0.1842\nEpoch 7/47\n456/456 [==============================] - 77s 169ms/step - loss: 3.9076 - accuracy: 0.2309 - val_loss: 4.1024 - val_accuracy: 0.2058\nEpoch 8/47\n456/456 [==============================] - 76s 167ms/step - loss: 3.6295 - accuracy: 0.2807 - val_loss: 3.8681 - val_accuracy: 0.2329\nEpoch 9/47\n456/456 [==============================] - 76s 166ms/step - loss: 3.4035 - accuracy: 0.3169 - val_loss: 4.1603 - val_accuracy: 0.1916\nEpoch 10/47\n456/456 [==============================] - 75s 165ms/step - loss: 3.2058 - accuracy: 0.3475 - val_loss: 2.8850 - val_accuracy: 0.3733\nEpoch 11/47\n456/456 [==============================] - 75s 165ms/step - loss: 3.0076 - accuracy: 0.3842 - val_loss: 3.3100 - val_accuracy: 0.3220\nEpoch 12/47\n456/456 [==============================] - 75s 164ms/step - loss: 2.8261 - accuracy: 0.4241 - val_loss: 5.1452 - val_accuracy: 0.1491\nEpoch 13/47\n456/456 [==============================] - 75s 165ms/step - loss: 2.6727 - accuracy: 0.4596 - val_loss: 5.0380 - val_accuracy: 0.1455\nEpoch 14/47\n456/456 [==============================] - 75s 165ms/step - loss: 2.5387 - accuracy: 0.4788 - val_loss: 3.6309 - val_accuracy: 0.2883\nEpoch 15/47\n456/456 [==============================] - 76s 166ms/step - loss: 2.3955 - accuracy: 0.5164 - val_loss: 2.5527 - val_accuracy: 0.4398\nEpoch 16/47\n456/456 [==============================] - 75s 165ms/step - loss: 2.2652 - accuracy: 0.5480 - val_loss: 2.7457 - val_accuracy: 0.4100\nEpoch 17/47\n456/456 [==============================] - 76s 167ms/step - loss: 2.1441 - accuracy: 0.5718 - val_loss: 3.4025 - val_accuracy: 0.3357\nEpoch 18/47\n456/456 [==============================] - 76s 166ms/step - loss: 2.0319 - accuracy: 0.5970 - val_loss: 3.0866 - val_accuracy: 0.3938\nEpoch 19/47\n456/456 [==============================] - 76s 166ms/step - loss: 1.9291 - accuracy: 0.6208 - val_loss: 3.3955 - val_accuracy: 0.3535\nEpoch 20/47\n456/456 [==============================] - 76s 166ms/step - loss: 1.8496 - accuracy: 0.6380 - val_loss: 2.2764 - val_accuracy: 0.5067\nEpoch 21/47\n456/456 [==============================] - 78s 171ms/step - loss: 1.7565 - accuracy: 0.6580 - val_loss: 2.4785 - val_accuracy: 0.4648\nEpoch 22/47\n456/456 [==============================] - 77s 168ms/step - loss: 1.6752 - accuracy: 0.6723 - val_loss: 2.5700 - val_accuracy: 0.4626\nEpoch 23/47\n456/456 [==============================] - 76s 166ms/step - loss: 1.5876 - accuracy: 0.6966 - val_loss: 2.3844 - val_accuracy: 0.4886\nEpoch 24/47\n456/456 [==============================] - 76s 166ms/step - loss: 1.5227 - accuracy: 0.7096 - val_loss: 1.5103 - val_accuracy: 0.6476\nEpoch 25/47\n456/456 [==============================] - 76s 167ms/step - loss: 1.4390 - accuracy: 0.7299 - val_loss: 1.5489 - val_accuracy: 0.6542\nEpoch 26/47\n456/456 [==============================] - 76s 166ms/step - loss: 1.3800 - accuracy: 0.7404 - val_loss: 2.1054 - val_accuracy: 0.5489\nEpoch 27/47\n456/456 [==============================] - 75s 165ms/step - loss: 1.3175 - accuracy: 0.7530 - val_loss: 1.5359 - val_accuracy: 0.6547\nEpoch 28/47\n456/456 [==============================] - 75s 166ms/step - loss: 1.2586 - accuracy: 0.7672 - val_loss: 1.6077 - val_accuracy: 0.6383\nEpoch 29/47\n456/456 [==============================] - 75s 165ms/step - loss: 1.2029 - accuracy: 0.7764 - val_loss: 1.5192 - val_accuracy: 0.6539\nEpoch 30/47\n456/456 [==============================] - 76s 166ms/step - loss: 1.1355 - accuracy: 0.7943 - val_loss: 1.4405 - val_accuracy: 0.6703\nEpoch 31/47\n456/456 [==============================] - 75s 165ms/step - loss: 1.0805 - accuracy: 0.8023 - val_loss: 1.5876 - val_accuracy: 0.6498\nEpoch 32/47\n456/456 [==============================] - 75s 165ms/step - loss: 1.0454 - accuracy: 0.8098 - val_loss: 1.5188 - val_accuracy: 0.6637\nEpoch 33/47\n456/456 [==============================] - 78s 170ms/step - loss: 0.9933 - accuracy: 0.8244 - val_loss: 3.0994 - val_accuracy: 0.4352\nEpoch 34/47\n456/456 [==============================] - 78s 171ms/step - loss: 0.9605 - accuracy: 0.8307 - val_loss: 1.4399 - val_accuracy: 0.6780\nEpoch 35/47\n456/456 [==============================] - 79s 172ms/step - loss: 0.9273 - accuracy: 0.8387 - val_loss: 1.4036 - val_accuracy: 0.6857\nEpoch 36/47\n456/456 [==============================] - 78s 172ms/step - loss: 0.8901 - accuracy: 0.8429 - val_loss: 1.6151 - val_accuracy: 0.6498\nEpoch 37/47\n456/456 [==============================] - 78s 172ms/step - loss: 0.8468 - accuracy: 0.8481 - val_loss: 1.1726 - val_accuracy: 0.7405\nEpoch 38/47\n456/456 [==============================] - 78s 171ms/step - loss: 0.8033 - accuracy: 0.8627 - val_loss: 1.2133 - val_accuracy: 0.7273\nEpoch 39/47\n456/456 [==============================] - 78s 171ms/step - loss: 0.7726 - accuracy: 0.8695 - val_loss: 1.2021 - val_accuracy: 0.7328\nEpoch 40/47\n456/456 [==============================] - 79s 174ms/step - loss: 0.7461 - accuracy: 0.8731 - val_loss: 1.8503 - val_accuracy: 0.6199\nEpoch 41/47\n456/456 [==============================] - 79s 174ms/step - loss: 0.7107 - accuracy: 0.8796 - val_loss: 1.4731 - val_accuracy: 0.6763\nEpoch 42/47\n456/456 [==============================] - 79s 174ms/step - loss: 0.6886 - accuracy: 0.8828 - val_loss: 1.3949 - val_accuracy: 0.7040\nEpoch 43/47\n456/456 [==============================] - 79s 173ms/step - loss: 0.6499 - accuracy: 0.8966 - val_loss: 1.5127 - val_accuracy: 0.6742\nEpoch 44/47\n456/456 [==============================] - 80s 175ms/step - loss: 0.6488 - accuracy: 0.8936 - val_loss: 1.2983 - val_accuracy: 0.7112\nEpoch 45/47\n456/456 [==============================] - 79s 173ms/step - loss: 0.6076 - accuracy: 0.8999 - val_loss: 1.2204 - val_accuracy: 0.7306\nEpoch 46/47\n456/456 [==============================] - 80s 176ms/step - loss: 0.5809 - accuracy: 0.9089 - val_loss: 1.4318 - val_accuracy: 0.6972\nEpoch 47/47\n456/456 [==============================] - 79s 174ms/step - loss: 0.5753 - accuracy: 0.9044 - val_loss: 1.3958 - val_accuracy: 0.7018\n"
    }
   ],
   "source": [
    "# I commented out some of the code for learning the model.\n",
    "def main():\n",
    "    train_dict = reader('dataset_seperate/train/')\n",
    "    X_train = np.array(train_dict['image'])\n",
    "    y_train = to_categorical(np.array(train_dict['label']))\n",
    "\n",
    "    train_model(X_train, y_train)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&#39;2.2.0&#39;"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}