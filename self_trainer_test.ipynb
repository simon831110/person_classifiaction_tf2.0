{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "a4ca0a1be38340d2e1b70bc6cc73081324f38c92d6b10994d2eeb96728463324"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense,Flatten,Activation,Dropout,Maximum,ZeroPadding2D,Conv2D,MaxPooling2D,concatenate,Add,BatchNormalization,LeakyReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize as imresize\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layers set\n",
    "def dense_set(inp_layer, n, activation, drop_rate=0.):\n",
    "    dp = Dropout(drop_rate)(inp_layer)\n",
    "    dns = Dense(n)(dp)\n",
    "    bn = BatchNormalization(axis=-1)(dns)\n",
    "    act = Activation(activation=activation)(bn)\n",
    "    return act\n",
    "\n",
    "# Conv layers set\n",
    "def conv_layer(feature_batch, feature_map, kernel_size=(3, 3),strides=(1,1), zp_flag=False):\n",
    "    if zp_flag:\n",
    "        zp = ZeroPadding2D((1,1))(feature_batch)\n",
    "    else:\n",
    "        zp = feature_batch\n",
    "    conv = Conv2D(filters=feature_map, kernel_size=kernel_size, strides=strides,padding='same')(zp)\n",
    "    bn = BatchNormalization(axis=3)(conv)\n",
    "    act = LeakyReLU(1/10)(bn)\n",
    "    return act\n",
    "# residual_block\n",
    "def residual_block(feature_batch, feature_map,stride=(1,1)):\n",
    "    res=conv_layer(feature_batch, feature_map, kernel_size=(3, 3),strides=stride, zp_flag=False)\n",
    "    res=conv_layer(res, feature_map, kernel_size=(3, 3),strides=(1,1), zp_flag=False)\n",
    "    shortcut=Conv2D(filters=feature_map, kernel_size=(3,3), strides=stride,padding='same')(feature_batch)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    output=Add()([shortcut,res])\n",
    "    output=LeakyReLU(1/10)(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model \n",
    "def get_model():\n",
    "    inp_img = Input(shape=(128, 64, 3))\n",
    "\n",
    "    # 32\n",
    "    conv1 = residual_block(inp_img, 32)\n",
    "    conv2 = residual_block(conv1, 32)\n",
    "    # 64\n",
    "    conv3 = residual_block(conv2, 64,stride=(2,2))\n",
    "    conv4 = residual_block(conv3, 64)\n",
    "    # 128\n",
    "    conv5 = residual_block(conv4, 128,stride=(2,2))\n",
    "    conv6 = residual_block(conv5, 128)\n",
    "    conv7 = residual_block(conv6, 128)\n",
    "    mp3 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2),padding='same')(conv7)\n",
    "\n",
    "    flt=tf.keras.layers.GlobalAveragePooling2D()(mp3)\n",
    "    ds1 = dense_set(flt, 128, activation='tanh')\n",
    "    feature=tf.math.l2_normalize(ds1, axis=1)\n",
    "    out = dense_set(ds1, 1500, activation='softmax')\n",
    "\n",
    "    model = Model(inputs=inp_img, outputs=out)\n",
    "    \n",
    "    # The first 50 epochs are used by Adam opt.\n",
    "    # Then 30 epochs are used by SGD opt.\n",
    "    \n",
    "    #mypotim = Adam(lr=2 * 1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    mypotim = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=mypotim,\n",
    "                   metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(img, label):\n",
    "    gmodel = get_model()\n",
    "    gmodel.load_weights(filepath='model/model.47-1.00.hdf5')\n",
    "    prob = gmodel.predict(img, verbose=1)\n",
    "    pred = prob.argmax(axis=-1)\n",
    "    #sub = pd.DataFrame({\"file\": label,\n",
    "    #                     \"species\": [INV_CLASS[p] for p in pred]})\n",
    "    #sub.to_csv(\"sub.csv\", index=False, header=True)\n",
    "    pred=np.array(pred)\n",
    "\n",
    "    count=0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i]==label[i]:\n",
    "            count+=1\n",
    "    print(float(count/len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Resize all image to 128x64\n",
    "def img_reshape(img):\n",
    "    img = imresize(img, (128, 64, 3))\n",
    "    return img\n",
    "\n",
    "# get image tag\n",
    "def img_label(path):\n",
    "    path=path.split('/')[-1]\n",
    "    return int(re.split(r'\\\\',path)[-2])\n",
    "\n",
    "# fill train and test dict\n",
    "def fill_dict(paths, some_dict):\n",
    "    text = ''\n",
    "    text = 'Start fill test_dict'\n",
    "    \n",
    "    for p in tqdm(paths, ascii=True, ncols=85, desc=text):\n",
    "        img = imageio.imread(p)\n",
    "        img = img_reshape(img)\n",
    "        some_dict['image'].append(img)\n",
    "        some_dict['label'].append(img_label(p))\n",
    "\n",
    "    return some_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image from dir. and fill test dict\n",
    "def reader():\n",
    "    file_ext = []\n",
    "    train_path = []\n",
    "    test_path = []\n",
    "\n",
    "    for root, dirs, files in os.walk('dataset_seperate/test/'):\n",
    "        if dirs != []:\n",
    "            print('Root:\\n'+str(root))\n",
    "            print('Dirs:\\n'+str(dirs))\n",
    "        else:\n",
    "            for f in files:\n",
    "                ext = os.path.splitext(str(f))[1][1:]\n",
    "                if ext not in file_ext:\n",
    "                    file_ext.append(ext)\n",
    "                path = os.path.join(root, f)\n",
    "                test_path.append(path)\n",
    "    test_dict = {\n",
    "        'image': [],\n",
    "        'label': []\n",
    "    }\n",
    "    test_dict = fill_dict(test_path, test_dict)\n",
    "    return  test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1337&#39;, &#39;1338&#39;, &#39;1339&#39;, &#39;1340&#39;, &#39;1341&#39;, &#39;1342&#39;, &#39;1343&#39;, &#39;1344&#39;, &#39;1345&#39;, &#39;1346&#39;, &#39;1347&#39;, &#39;1348&#39;, &#39;1349&#39;, &#39;1350&#39;, &#39;1351&#39;, &#39;1352&#39;, &#39;1353&#39;, &#39;1354&#39;, &#39;1355&#39;, &#39;1356&#39;, &#39;1357&#39;, &#39;1358&#39;, &#39;1359&#39;, &#39;1360&#39;, &#39;1361&#39;, &#39;1362&#39;, &#39;1363&#39;, &#39;1364&#39;, &#39;1365&#39;, &#39;1366&#39;, &#39;1367&#39;, &#39;1368&#39;, &#39;1369&#39;, &#39;1370&#39;, &#39;1371&#39;, &#39;1372&#39;, &#39;1373&#39;, &#39;1374&#39;, &#39;1375&#39;, &#39;1376&#39;, &#39;1377&#39;, &#39;1378&#39;, &#39;1379&#39;, &#39;1380&#39;, &#39;1381&#39;, &#39;1382&#39;, &#39;1383&#39;, &#39;1384&#39;, &#39;1385&#39;, &#39;1386&#39;, &#39;1387&#39;, &#39;1388&#39;, &#39;1389&#39;, &#39;1390&#39;, &#39;1391&#39;, &#39;1392&#39;, &#39;1393&#39;, &#39;1394&#39;, &#39;1395&#39;, &#39;1396&#39;, &#39;1397&#39;, &#39;1398&#39;, &#39;1399&#39;, &#39;1400&#39;, &#39;1401&#39;, &#39;1402&#39;, &#39;1403&#39;, &#39;1404&#39;, &#39;1405&#39;, &#39;1406&#39;, &#39;1407&#39;, &#39;1408&#39;, &#39;1409&#39;, &#39;1410&#39;, &#39;1411&#39;, &#39;1412&#39;, &#39;1413&#39;, &#39;1414&#39;, &#39;1415&#39;, &#39;1416&#39;, &#39;1417&#39;, &#39;1418&#39;, &#39;1419&#39;, &#39;1420&#39;, &#39;1421&#39;, &#39;1422&#39;, &#39;1423&#39;, &#39;1424&#39;, &#39;1425&#39;, &#39;1426&#39;, &#39;1427&#39;, &#39;1428&#39;, &#39;1429&#39;, &#39;1430&#39;, &#39;1431&#39;, &#39;1432&#39;, &#39;1433&#39;, &#39;1434&#39;, &#39;1435&#39;, &#39;1436&#39;, &#39;1437&#39;, &#39;1438&#39;, &#39;1439&#39;, &#39;1440&#39;, &#39;1441&#39;, &#39;1442&#39;, &#39;1443&#39;, &#39;1444&#39;, &#39;1445&#39;, &#39;1446&#39;, &#39;1447&#39;, &#39;1448&#39;, &#39;1449&#39;, &#39;1450&#39;, &#39;1451&#39;, &#39;1452&#39;, &#39;1453&#39;, &#39;1454&#39;, &#39;1455&#39;, &#39;1456&#39;, &#39;1457&#39;, &#39;1458&#39;, &#39;1459&#39;, &#39;1460&#39;, &#39;1461&#39;, &#39;1462&#39;, &#39;1463&#39;, &#39;1464&#39;, &#39;1465&#39;, &#39;1466&#39;, &#39;1467&#39;, &#39;1468&#39;, &#39;1469&#39;, &#39;1470&#39;, &#39;1471&#39;, &#39;1472&#39;, &#39;1473&#39;, &#39;1474&#39;, &#39;1475&#39;, &#39;1476&#39;, &#39;1477&#39;, &#39;1478&#39;, &#39;1479&#39;, &#39;1480&#39;, &#39;1481&#39;, &#39;1482&#39;, &#39;1483&#39;, &#39;1484&#39;, &#39;1485&#39;, &#39;1486&#39;, &#39;1487&#39;, &#39;1488&#39;, &#39;1489&#39;, &#39;1490&#39;, &#39;1491&#39;, &#39;1492&#39;, &#39;1493&#39;, &#39;1494&#39;, &#39;1495&#39;, &#39;1496&#39;, &#39;1497&#39;, &#39;1498&#39;, &#39;1499&#39;]\nStart fill test_dict: 100%|#####################| 7751/7751 [00:32&lt;00:00, 241.68it/s]\nModel: &quot;model&quot;\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 128, 64, 3)] 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 128, 64, 32)  896         input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 128, 64, 32)  128         conv2d[0][0]                     \n__________________________________________________________________________________________________\nleaky_re_lu (LeakyReLU)         (None, 128, 64, 32)  0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 128, 64, 32)  9248        leaky_re_lu[0][0]                \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 128, 64, 32)  896         input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 128, 64, 32)  128         conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 128, 64, 32)  128         conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)       (None, 128, 64, 32)  0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nadd (Add)                       (None, 128, 64, 32)  0           batch_normalization_2[0][0]      \n                                                                 leaky_re_lu_1[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)       (None, 128, 64, 32)  0           add[0][0]                        \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 128, 64, 32)  9248        leaky_re_lu_2[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 128, 64, 32)  128         conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)       (None, 128, 64, 32)  0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 128, 64, 32)  9248        leaky_re_lu_3[0][0]              \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 128, 64, 32)  9248        leaky_re_lu_2[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 128, 64, 32)  128         conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 128, 64, 32)  128         conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)       (None, 128, 64, 32)  0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 128, 64, 32)  0           batch_normalization_5[0][0]      \n                                                                 leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)       (None, 128, 64, 32)  0           add_1[0][0]                      \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 64, 32, 64)   18496       leaky_re_lu_5[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 64, 32, 64)   256         conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)       (None, 64, 32, 64)   0           batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 64, 32, 64)   36928       leaky_re_lu_6[0][0]              \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 64, 32, 64)   18496       leaky_re_lu_5[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 64, 32, 64)   256         conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 64, 32, 64)   256         conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)       (None, 64, 32, 64)   0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 64, 32, 64)   0           batch_normalization_8[0][0]      \n                                                                 leaky_re_lu_7[0][0]              \n__________________________________________________________________________________________________\nleaky_re_lu_8 (LeakyReLU)       (None, 64, 32, 64)   0           add_2[0][0]                      \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 64, 32, 64)   36928       leaky_re_lu_8[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 64, 32, 64)   256         conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_9 (LeakyReLU)       (None, 64, 32, 64)   0           batch_normalization_9[0][0]      \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 64, 32, 64)   36928       leaky_re_lu_9[0][0]              \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 64, 32, 64)   36928       leaky_re_lu_8[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 64, 32, 64)   256         conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 64, 32, 64)   256         conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_10 (LeakyReLU)      (None, 64, 32, 64)   0           batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 64, 32, 64)   0           batch_normalization_11[0][0]     \n                                                                 leaky_re_lu_10[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_11 (LeakyReLU)      (None, 64, 32, 64)   0           add_3[0][0]                      \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 32, 16, 128)  73856       leaky_re_lu_11[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 32, 16, 128)  512         conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_12 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 32, 16, 128)  73856       leaky_re_lu_11[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 32, 16, 128)  512         conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 32, 16, 128)  512         conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_13 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\nadd_4 (Add)                     (None, 32, 16, 128)  0           batch_normalization_14[0][0]     \n                                                                 leaky_re_lu_13[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_14 (LeakyReLU)      (None, 32, 16, 128)  0           add_4[0][0]                      \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_14[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 32, 16, 128)  512         conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_15 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_15[0][0]             \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_14[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 32, 16, 128)  512         conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 32, 16, 128)  512         conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_16 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\nadd_5 (Add)                     (None, 32, 16, 128)  0           batch_normalization_17[0][0]     \n                                                                 leaky_re_lu_16[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_17 (LeakyReLU)      (None, 32, 16, 128)  0           add_5[0][0]                      \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_17[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 32, 16, 128)  512         conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_18 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_18[0][0]     \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_18[0][0]             \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 32, 16, 128)  147584      leaky_re_lu_17[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 32, 16, 128)  512         conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 32, 16, 128)  512         conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_19 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_19[0][0]     \n__________________________________________________________________________________________________\nadd_6 (Add)                     (None, 32, 16, 128)  0           batch_normalization_20[0][0]     \n                                                                 leaky_re_lu_19[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_20 (LeakyReLU)      (None, 32, 16, 128)  0           add_6[0][0]                      \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 16, 8, 128)   0           leaky_re_lu_20[0][0]             \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 128)          0           max_pooling2d[0][0]              \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 128)          0           global_average_pooling2d[0][0]   \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 128)          16512       dropout[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_21 (BatchNo (None, 128)          512         dense[0][0]                      \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 128)          0           batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 128)          0           activation[0][0]                 \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1500)         193500      dropout_1[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_22 (BatchNo (None, 1500)         6000        dense_1[0][0]                    \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 1500)         0           batch_normalization_22[0][0]     \n==================================================================================================\nTotal params: 1,627,724\nTrainable params: 1,621,012\nNon-trainable params: 6,712\n__________________________________________________________________________________________________\n243/243 [==============================] - 16s 67ms/step\n0.8125403173784028\n"
    }
   ],
   "source": [
    "# I commented out some of the code for learning the model.\n",
    "def main():\n",
    "    test_dict = reader()\n",
    "    X_test = np.array(test_dict['image'])\n",
    "    label = np.array(test_dict['label'])\n",
    "    # I do not recommend trying to train the model on a kaggle.\n",
    "    #train_model(X_train, y_train)\n",
    "    test_model(X_test, label)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ]
}