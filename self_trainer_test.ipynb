{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597399249643",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Maximum\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize as imresize\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layers set\n",
    "def dense_set(inp_layer, n, activation, drop_rate=0.):\n",
    "    dp = Dropout(drop_rate)(inp_layer)\n",
    "    dns = Dense(n)(dp)\n",
    "    bn = BatchNormalization(axis=-1)(dns)\n",
    "    act = Activation(activation=activation)(bn)\n",
    "    return act\n",
    "\n",
    "# Conv. layers set\n",
    "def conv_layer(feature_batch, feature_map, kernel_size=(3, 3),strides=(1,1), zp_flag=False):\n",
    "    if zp_flag:\n",
    "        zp = ZeroPadding2D((1,1))(feature_batch)\n",
    "    else:\n",
    "        zp = feature_batch\n",
    "    conv = Conv2D(filters=feature_map, kernel_size=kernel_size, strides=strides,padding='same')(zp)\n",
    "    bn = BatchNormalization(axis=3)(conv)\n",
    "    act = LeakyReLU(1/10)(bn)\n",
    "    return act\n",
    "def residual_block(feature_batch, feature_map,stride=(1,1)):\n",
    "    res=conv_layer(feature_batch, feature_map, kernel_size=(3, 3),strides=stride, zp_flag=False)\n",
    "    res=conv_layer(res, feature_map, kernel_size=(3, 3),strides=(1,1), zp_flag=False)\n",
    "    shortcut=Conv2D(filters=feature_map, kernel_size=(3,3), strides=stride,padding='same')(feature_batch)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    output=Add()([shortcut,res])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model \n",
    "def get_model():\n",
    "    inp_img = Input(shape=(128, 64, 3))\n",
    "\n",
    "    # 51\n",
    "    conv1 = residual_block(inp_img, 64)\n",
    "    conv2 = residual_block(conv1, 64)\n",
    "    #mp1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2),padding='same')(conv2)      #64*32\n",
    "    # 23\n",
    "    conv3 = residual_block(conv2, 128,stride=(2,2))\n",
    "    conv4 = residual_block(conv3, 128)\n",
    "    #mp2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2),padding='same')(conv4)      #32*16\n",
    "    # 9\n",
    "    conv7 = residual_block(conv4, 256,stride=(2,2))\n",
    "    conv8 = residual_block(conv7, 256)\n",
    "    conv9 = residual_block(conv8, 256)\n",
    "    mp3 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2),padding='same')(conv9)      #16*8\n",
    "    # 1\n",
    "    # dense layers\n",
    "    flt=tf.keras.layers.GlobalAveragePooling2D()(mp3)\n",
    "    #flt = Flatten()(mp3)\n",
    "    ds1 = dense_set(flt, 128, activation='tanh')\n",
    "    feature=tf.math.l2_normalize(ds1, axis=1)\n",
    "    out = dense_set(ds1, 1500, activation='softmax')\n",
    "\n",
    "    model = Model(inputs=inp_img, outputs=out)\n",
    "    \n",
    "    # The first 50 epochs are used by Adam opt.\n",
    "    # Then 30 epochs are used by SGD opt.\n",
    "    \n",
    "    mypotim = Adam(lr=2 * 1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    #mypotim = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=mypotim,\n",
    "                   metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I trained model about 12h on GTX 950.\n",
    "def train_model(img, target):\n",
    "    my_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model/model.{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    #gmodel = get_model()\n",
    "    #gmodel.load_weights(filepath='model/model.03-6.29.hdf5')    #載入預先訓練的權重\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "                                                        img,\n",
    "                                                        target,\n",
    "                                                        shuffle=True,\n",
    "                                                        train_size=0.8,\n",
    "                                                        random_state=1987\n",
    "                                                        )\n",
    "    gen = ImageDataGenerator(\n",
    "            #rotation_range=360.,\n",
    "            #width_shift_range=0.3,\n",
    "            #height_shift_range=0.3,\n",
    "            #zoom_range=0.3,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True\n",
    "    )\n",
    "    #show augumented images\n",
    "    show_img=image.img_to_array(x_train[0])\n",
    "    show_img=show_img.reshape((1,)+show_img.shape)\n",
    "    i=0\n",
    "    for batch in gen.flow(show_img,save_prefix='test',save_format='jpeg'):\n",
    "            plt.figure(i)\n",
    "            plot=plt.imshow(image.img_to_array(batch[0]))\n",
    "            i+=1\n",
    "            if i>4:\n",
    "                break\n",
    "    plt.show()\n",
    "    '''\n",
    "    gmodel.fit(gen.flow(x_train, y_train,batch_size=32),\n",
    "               steps_per_epoch=len(x_train)/32,\n",
    "               epochs=47,\n",
    "               verbose=1,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_valid, y_valid),\n",
    "               callbacks=my_callbacks\n",
    "               )\n",
    "    '''\n",
    "    #gmodel.save(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(img, label):\n",
    "    gmodel = get_model()\n",
    "    gmodel.load_weights(filepath='model/model.01-6.95.hdf5')\n",
    "    prob = gmodel.predict(img, verbose=1)\n",
    "    pred = prob.argmax(axis=-1)\n",
    "    #sub = pd.DataFrame({\"file\": label,\n",
    "    #                     \"species\": [INV_CLASS[p] for p in pred]})\n",
    "    #sub.to_csv(\"sub.csv\", index=False, header=True)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Resize all image to 51x51 \n",
    "def img_reshape(img):\n",
    "    img = imresize(img, (128, 64, 3))\n",
    "    return img\n",
    "\n",
    "# get image tag\n",
    "def img_label(path):\n",
    "    return int(re.split(r'\\\\',path)[-2])\n",
    "\n",
    "# fill train and test dict\n",
    "def fill_dict(paths, some_dict):\n",
    "    text = ''\n",
    "    text = 'Start fill test_dict'\n",
    "    \n",
    "    for p in tqdm(paths, ascii=True, ncols=85, desc=text):\n",
    "        img = imageio.imread(p)\n",
    "        img = img_reshape(img)\n",
    "        some_dict['image'].append(img)\n",
    "        some_dict['label'].append(0)\n",
    "\n",
    "    return some_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image from dir. and fill train and test dict\n",
    "def reader():\n",
    "    file_ext = []\n",
    "    train_path = []\n",
    "    test_path = []\n",
    "\n",
    "    for root, dirs, files in os.walk('test'):\n",
    "        if dirs != []:\n",
    "            print('Root:\\n'+str(root))\n",
    "            print('Dirs:\\n'+str(dirs))\n",
    "        else:\n",
    "            for f in files:\n",
    "                ext = os.path.splitext(str(f))[1][1:]\n",
    "\n",
    "                if ext not in file_ext:\n",
    "                    file_ext.append(ext)\n",
    "                path = os.path.join(root, f)\n",
    "                print(path)\n",
    "                test_path.append(path)\n",
    "    train_dict = {\n",
    "        'image': [],\n",
    "        'label': []\n",
    "    }\n",
    "    test_dict = {\n",
    "        'image': [],\n",
    "        'label': []\n",
    "    }\n",
    "    #train_dict = fill_dict(train_path, train_dict)\n",
    "    test_dict = fill_dict(test_path, test_dict)\n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Start fill test_dict:  22%|######2                     | 2/9 [00:00<00:00, 19.80it/s]test\\1500c1T0000F0012927.jpg\ntest\\1500c1T0000F0012928.jpg\ntest\\1500c3T0000F0012929.jpg\ntest\\1500c3T0000F0012930.jpg\ntest\\1500c3T0000F0012931.jpg\ntest\\1500c5T0000F0012932.jpg\ntest\\1500c5T0000F0012933.jpg\ntest\\1500c6T0000F0012934.jpg\ntest\\1500c6T0000F0012935.jpg\nStart fill test_dict: 100%|############################| 9/9 [00:00<00:00, 56.23it/s]\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 128, 64, 3)] 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 128, 64, 64)  1792        input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 128, 64, 64)  256         conv2d[0][0]                     \n__________________________________________________________________________________________________\nleaky_re_lu (LeakyReLU)         (None, 128, 64, 64)  0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 128, 64, 64)  36928       leaky_re_lu[0][0]                \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 128, 64, 64)  1792        input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 128, 64, 64)  256         conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 128, 64, 64)  256         conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)       (None, 128, 64, 64)  0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nadd (Add)                       (None, 128, 64, 64)  0           batch_normalization_2[0][0]      \n                                                                 leaky_re_lu_1[0][0]              \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 128, 64, 64)  36928       add[0][0]                        \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 128, 64, 64)  256         conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)       (None, 128, 64, 64)  0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 128, 64, 64)  36928       leaky_re_lu_2[0][0]              \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 128, 64, 64)  36928       add[0][0]                        \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 128, 64, 64)  256         conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 128, 64, 64)  256         conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)       (None, 128, 64, 64)  0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 128, 64, 64)  0           batch_normalization_5[0][0]      \n                                                                 leaky_re_lu_3[0][0]              \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 64, 32, 128)  73856       add_1[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 64, 32, 128)  512         conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)       (None, 64, 32, 128)  0           batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 64, 32, 128)  147584      leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 64, 32, 128)  73856       add_1[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 64, 32, 128)  512         conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 64, 32, 128)  512         conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)       (None, 64, 32, 128)  0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 64, 32, 128)  0           batch_normalization_8[0][0]      \n                                                                 leaky_re_lu_5[0][0]              \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 64, 32, 128)  147584      add_2[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 64, 32, 128)  512         conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)       (None, 64, 32, 128)  0           batch_normalization_9[0][0]      \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 64, 32, 128)  147584      leaky_re_lu_6[0][0]              \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 64, 32, 128)  147584      add_2[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 64, 32, 128)  512         conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 64, 32, 128)  512         conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)       (None, 64, 32, 128)  0           batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 64, 32, 128)  0           batch_normalization_11[0][0]     \n                                                                 leaky_re_lu_7[0][0]              \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 32, 16, 256)  295168      add_3[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 32, 16, 256)  1024        conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_8 (LeakyReLU)       (None, 32, 16, 256)  0           batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 32, 16, 256)  590080      leaky_re_lu_8[0][0]              \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 32, 16, 256)  295168      add_3[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 32, 16, 256)  1024        conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 32, 16, 256)  1024        conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_9 (LeakyReLU)       (None, 32, 16, 256)  0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\nadd_4 (Add)                     (None, 32, 16, 256)  0           batch_normalization_14[0][0]     \n                                                                 leaky_re_lu_9[0][0]              \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 32, 16, 256)  590080      add_4[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 32, 16, 256)  1024        conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_10 (LeakyReLU)      (None, 32, 16, 256)  0           batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 32, 16, 256)  590080      leaky_re_lu_10[0][0]             \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 32, 16, 256)  590080      add_4[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 32, 16, 256)  1024        conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 32, 16, 256)  1024        conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_11 (LeakyReLU)      (None, 32, 16, 256)  0           batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\nadd_5 (Add)                     (None, 32, 16, 256)  0           batch_normalization_17[0][0]     \n                                                                 leaky_re_lu_11[0][0]             \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 32, 16, 256)  590080      add_5[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 32, 16, 256)  1024        conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_12 (LeakyReLU)      (None, 32, 16, 256)  0           batch_normalization_18[0][0]     \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 32, 16, 256)  590080      leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 32, 16, 256)  590080      add_5[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 32, 16, 256)  1024        conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 32, 16, 256)  1024        conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_13 (LeakyReLU)      (None, 32, 16, 256)  0           batch_normalization_19[0][0]     \n__________________________________________________________________________________________________\nadd_6 (Add)                     (None, 32, 16, 256)  0           batch_normalization_20[0][0]     \n                                                                 leaky_re_lu_13[0][0]             \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 16, 8, 256)   0           add_6[0][0]                      \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 256)          0           max_pooling2d[0][0]              \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 256)          0           global_average_pooling2d[0][0]   \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 128)          32896       dropout[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_21 (BatchNo (None, 128)          512         dense[0][0]                      \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 128)          0           batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 128)          0           activation[0][0]                 \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1500)         193500      dropout_1[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_22 (BatchNo (None, 1500)         6000        dense_1[0][0]                    \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 1500)         0           batch_normalization_22[0][0]     \n==================================================================================================\nTotal params: 5,856,972\nTrainable params: 5,846,804\nNon-trainable params: 10,168\n__________________________________________________________________________________________________\n1/1 [==============================] - 0s 999us/step\n[ 132  963  132  963  132  963  963 1393 1393]\n"
    }
   ],
   "source": [
    "# I commented out some of the code for learning the model.\n",
    "def main():\n",
    "    train_dict, test_dict = reader()\n",
    "    #X_train = np.array(train_dict['image'])\n",
    "    #y_train = to_categorical(np.array(train_dict['label']))\n",
    "\n",
    "    X_test = np.array(test_dict['image'])\n",
    "    label = test_dict['label']\n",
    "    \n",
    "    # I do not recommend trying to train the model on a kaggle.\n",
    "    #train_model(X_train, y_train)\n",
    "    test_model(X_test, label)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}